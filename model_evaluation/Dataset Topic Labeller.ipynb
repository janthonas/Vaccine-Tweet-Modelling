{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc1947-6d0c-4e90-b0dd-29bbdb812713",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebcfce61-c4e5-4e3a-b9a2-625db314c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "# Spacy's english model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# used to fix contractions such as I'll to I will\n",
    "import contractions\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41210710-f5c6-4a91-9b39-ef5f0dacc55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "tweet_data_fp = '../model_evaluation/Results/pruned_labelled_medicine_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20e4637e-c43e-4203-85df-11dab348d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_df = pd.read_csv(tweet_data_fp)\n",
    "tweet_data_df = tweet_data_df[tweet_data_df['Lang_code'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f50d1e-5014-4634-ae02-ea1368519dfc",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35cf1290-6af1-4d3b-8333-def84f2f8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering documents\n",
    "docs = tweet_data_df['Tweet'].tolist()\n",
    "pruned_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bb0cd1c-3181-4b56-bdb3-4ee6db32cee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ed1607c-d69e-415b-adeb-e05dc843075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best to err on the side of caution when pre-processing:\n",
    "# https://maartengr.github.io/BERTopic/faq.html#should-i-preprocess-the-data\n",
    "\n",
    "# Removal of links\n",
    "def remove_urls(doc):\n",
    "    return re.sub(r'http\\S+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_urls(doc) for doc in pruned_docs]\n",
    "\n",
    "# Converting hashtags into words\n",
    "# Or removing the hashtag and word entirely\n",
    "# Hashtags are converted to standard words as part of the tokenizer\n",
    "def remove_hashtags(doc):\n",
    "    return doc.replace('#', '')\n",
    "    #return re.sub(r'#\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_hashtags(doc) for doc in pruned_docs]\n",
    "\n",
    "# Converting mentions into words\n",
    "def remove_numbers(doc):\n",
    "    return re.sub(r'\\d+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_numbers(doc) for doc in pruned_docs]\n",
    "\n",
    "# Function to remove user mentions\n",
    "def remove_user_mentions(doc):\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_user_mentions(doc) for doc in pruned_docs]\n",
    "\n",
    "def fix_contractions(doc):\n",
    "    return contractions.fix(doc)\n",
    "\n",
    "pruned_docs = [fix_contractions(doc) for doc in pruned_docs]\n",
    "\n",
    "# Remove punctuation \n",
    "# Punctuation is removed as part of the tokenizer\n",
    "def remove_punctuation(doc):\n",
    "    return re.sub(r'[^\\w\\s]', '', doc)\n",
    "\n",
    "pruned_docs = [remove_punctuation(doc) for doc in pruned_docs]\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(doc):\n",
    "    return ' '.join([word for word in doc.split() if word.lower() not in stop_words])\n",
    "\n",
    "pruned_docs = [remove_stopwords(doc) for doc in pruned_docs]\n",
    "\n",
    "# Lemmatize the text\n",
    "def lemmatize_text(doc):\n",
    "    doc = nlp(doc)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.lemma_ not in stop_words and not token.is_punct and not token.is_space])\n",
    "\n",
    "pruned_docs = [lemmatize_text(doc) for doc in pruned_docs]\n",
    "\n",
    "# Remove the 'amp' word\n",
    "def remove_amp(doc):\n",
    "    return re.sub(r'\\bamp\\b', '', doc).strip() # strip removes the surrounding white space\n",
    "\n",
    "pruned_docs = [remove_amp(doc) for doc in pruned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3147f92c-2101-4fe0-bcf6-dc8c408db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
    "\n",
    "pruned_docs_split = [ tokenizer(doc) for doc in  pruned_docs ]\n",
    "\n",
    "# Combine the inner lists into sentences\n",
    "pruned_docs_tokenized = [' '.join(words) for words in pruned_docs_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b78a7-8ee7-4100-8410-d2cf83d66387",
   "metadata": {},
   "source": [
    "## Performing BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3121e0c8-5448-4874-85ae-fde499379e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modelling\n",
    "bertopic_model = BERTopic()\n",
    "topics, probs = bertopic_model.fit_transform(pruned_docs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ac83666-1e5e-4404-bcf4-9877861a5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_topic_df = bertopic_model.get_document_info(pruned_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da95ce8-7922-42cd-9cd6-d1f70426d588",
   "metadata": {},
   "source": [
    "## Calculating Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e761351f-985a-4bda-a09b-0b1545a05844",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = bertopic_model.get_topic_info()\n",
    "topics = topic_info['Representation'].tolist()\n",
    "\n",
    "word2id = Dictionary(pruned_docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d11fb7fe-25dd-4cc9-a3d5-11e2500238dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(topics=topics, \n",
    "                    texts=pruned_docs_split,\n",
    "                    coherence='c_npmi',  # c_npmi was used in the bertopic serbian research paper\n",
    "                    dictionary=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e89db08-b06e-4b3f-b075-e2a1f534397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "macro_topic_coherence = sum(coherence_per_topic) / len(coherence_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78bc0ada-aee0-4a05-9084-4f00b30d6e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1039039133490847"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This returns 0.02 when set to npmi, the serbian research return -0.042 as their best score\n",
    "# This returns 0.511 when set to c_v, this is a good coherence score\n",
    "# When removing non-english words and links this drops to 0.44\n",
    "# When adding lemmatization this goes up to 0.46\n",
    "# Removing numbers drop to 0.4567\n",
    "macro_topic_coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf5a62b5-367c-4119-9709-6ceb924b6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b7dd954-2bb0-4980-b48b-0fb3293ab78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e22d5a-8580-4fe0-b4b2-db4194560f66",
   "metadata": {},
   "source": [
    "## Calculating Topic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "957163b3-8bf3-4e62-b950-7759d5e65a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['covid',\n",
       "  'vaccine',\n",
       "  'say',\n",
       "  'people',\n",
       "  'pandemic',\n",
       "  'new',\n",
       "  'health',\n",
       "  'coronavirus',\n",
       "  'get',\n",
       "  'case'],\n",
       " ['china',\n",
       "  'origin',\n",
       "  'wuhan',\n",
       "  'virus',\n",
       "  'mission',\n",
       "  'theory',\n",
       "  'chinese',\n",
       "  'lab',\n",
       "  'outbreak',\n",
       "  'team'],\n",
       " ['maryland',\n",
       "  'smartnews',\n",
       "  'county',\n",
       "  'hogan',\n",
       "  'montgomery',\n",
       "  'md',\n",
       "  'gov',\n",
       "  'patch',\n",
       "  'case',\n",
       "  'covid'],\n",
       " ['israel',\n",
       "  'israeli',\n",
       "  'vaccination',\n",
       "  'covid',\n",
       "  'israelis',\n",
       "  'israels',\n",
       "  'vaccine',\n",
       "  'pfizer',\n",
       "  'show',\n",
       "  'study'],\n",
       " ['vaccine',\n",
       "  'country',\n",
       "  'donation',\n",
       "  'dose',\n",
       "  'donate',\n",
       "  'supply',\n",
       "  'million',\n",
       "  'vaccination',\n",
       "  'rich',\n",
       "  'us'],\n",
       " ['vaccine',\n",
       "  'covid',\n",
       "  'thread',\n",
       "  'covax',\n",
       "  'today',\n",
       "  'good',\n",
       "  'interesting',\n",
       "  'moderna',\n",
       "  'story',\n",
       "  'mrna'],\n",
       " ['briefing',\n",
       "  'press',\n",
       "  'covid',\n",
       "  'journalist',\n",
       "  'americas',\n",
       "  'america',\n",
       "  'cover',\n",
       "  'watch',\n",
       "  'update',\n",
       "  'situation'],\n",
       " ['latin',\n",
       "  'caribbean',\n",
       "  'america',\n",
       "  'paho',\n",
       "  'vaccine',\n",
       "  'country',\n",
       "  'region',\n",
       "  'director',\n",
       "  'etienne',\n",
       "  'brazil'],\n",
       " ['thread',\n",
       "  'good',\n",
       "  'interesting',\n",
       "  'scary',\n",
       "  'excellent',\n",
       "  'story',\n",
       "  'long',\n",
       "  'right',\n",
       "  'oop',\n",
       "  'befuddle'],\n",
       " ['haiti',\n",
       "  'earthquake',\n",
       "  'island',\n",
       "  'pan',\n",
       "  'gang',\n",
       "  'volcano',\n",
       "  'vincent',\n",
       "  'search',\n",
       "  'rescue',\n",
       "  'water'],\n",
       " ['omicron',\n",
       "  'variant',\n",
       "  'new',\n",
       "  'spread',\n",
       "  'delta',\n",
       "  'say',\n",
       "  'fact',\n",
       "  'global',\n",
       "  'wave',\n",
       "  'alpha'],\n",
       " ['science',\n",
       "  'story',\n",
       "  'scientist',\n",
       "  'interview',\n",
       "  'interesting',\n",
       "  'journalist',\n",
       "  'mcneill',\n",
       "  'reporter',\n",
       "  'dr',\n",
       "  'go'],\n",
       " ['sarscov',\n",
       "  'variant',\n",
       "  'virus',\n",
       "  'mutation',\n",
       "  'coronavirus',\n",
       "  'infectious',\n",
       "  'cause',\n",
       "  'increase',\n",
       "  'infect',\n",
       "  'concern'],\n",
       " ['interesting',\n",
       "  'probably',\n",
       "  'yes',\n",
       "  'fascinating',\n",
       "  'point',\n",
       "  'important',\n",
       "  'see',\n",
       "  'good',\n",
       "  '',\n",
       "  ''],\n",
       " ['death',\n",
       "  'die',\n",
       "  'country',\n",
       "  'case',\n",
       "  'people',\n",
       "  'pandemic',\n",
       "  'covid',\n",
       "  'surge',\n",
       "  'global',\n",
       "  'rise'],\n",
       " ['air',\n",
       "  'ventilation',\n",
       "  'indoor',\n",
       "  'window',\n",
       "  'car',\n",
       "  'building',\n",
       "  'hand',\n",
       "  'wear',\n",
       "  'particle',\n",
       "  'sanitizer'],\n",
       " ['eu',\n",
       "  'astrazeneca',\n",
       "  'vaccine',\n",
       "  'european',\n",
       "  'german',\n",
       "  'europe',\n",
       "  'merkel',\n",
       "  'export',\n",
       "  'hold',\n",
       "  'ema'],\n",
       " ['airborne',\n",
       "  'transmission',\n",
       "  'aerosol',\n",
       "  'sarscov',\n",
       "  'air',\n",
       "  'indoor',\n",
       "  'transmit',\n",
       "  'conclusion',\n",
       "  'exhale',\n",
       "  'error'],\n",
       " ['dose',\n",
       "  'billion',\n",
       "  'million',\n",
       "  'johnson',\n",
       "  'production',\n",
       "  'moderna',\n",
       "  'vaccine',\n",
       "  'company',\n",
       "  'pfizer',\n",
       "  'biontech'],\n",
       " ['impairment',\n",
       "  'job',\n",
       "  'worker',\n",
       "  'work',\n",
       "  'pandemic',\n",
       "  'care',\n",
       "  'leave',\n",
       "  'nurse',\n",
       "  'narrative',\n",
       "  'frailty'],\n",
       " ['survey',\n",
       "  'fallacy',\n",
       "  'prevent',\n",
       "  'vaccine',\n",
       "  'severe',\n",
       "  'people',\n",
       "  'would',\n",
       "  'death',\n",
       "  'expert',\n",
       "  'get'],\n",
       " ['facebook',\n",
       "  'user',\n",
       "  'film',\n",
       "  'instagram',\n",
       "  'privacy',\n",
       "  'social',\n",
       "  'teen',\n",
       "  'ad',\n",
       "  'apple',\n",
       "  'use'],\n",
       " ['euro',\n",
       "  'travel',\n",
       "  'europe',\n",
       "  'restriction',\n",
       "  'lift',\n",
       "  'cocaine',\n",
       "  'ban',\n",
       "  'curb',\n",
       "  'seize',\n",
       "  'germany'],\n",
       " ['symptom',\n",
       "  'trial',\n",
       "  'treatment',\n",
       "  'drug',\n",
       "  'long',\n",
       "  'infection',\n",
       "  'result',\n",
       "  'covid',\n",
       "  'child',\n",
       "  'ct'],\n",
       " ['mask',\n",
       "  'cloth',\n",
       "  'wear',\n",
       "  'surgical',\n",
       "  'double',\n",
       "  'doctor',\n",
       "  'ns',\n",
       "  'indoor',\n",
       "  'masks',\n",
       "  'kn'],\n",
       " ['trust',\n",
       "  'public',\n",
       "  'health',\n",
       "  'communication',\n",
       "  'risk',\n",
       "  'research',\n",
       "  'misinformation',\n",
       "  'acip',\n",
       "  'pandemic',\n",
       "  'communicate'],\n",
       " ['woman',\n",
       "  'violence',\n",
       "  'sexual',\n",
       "  'domestic',\n",
       "  'lifetime',\n",
       "  'racist',\n",
       "  'directory',\n",
       "  'taliban',\n",
       "  'endviolence',\n",
       "  'home'],\n",
       " ['mask',\n",
       "  'mandate',\n",
       "  'cdc',\n",
       "  'update',\n",
       "  'county',\n",
       "  'indoor',\n",
       "  'whether',\n",
       "  'public',\n",
       "  'guidance',\n",
       "  'new'],\n",
       " ['health',\n",
       "  'care',\n",
       "  'carissa',\n",
       "  'director',\n",
       "  'response',\n",
       "  'taxation',\n",
       "  'leader',\n",
       "  'etienne',\n",
       "  'religious',\n",
       "  'effective']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08be32fa-f492-449c-8ae9-fc3592fc61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def pairwise_jaccard_diversity(topics, topk=10):\n",
    "    '''\n",
    "    compute the average pairwise jaccard distance between the topics \n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed (There are 10 words in each topic)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pjd: average pairwise jaccard distance\n",
    "    '''\n",
    "    dist = 0\n",
    "    count = 0\n",
    "    for list1, list2 in combinations(topics, 2):\n",
    "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
    "        dist = dist + js\n",
    "        count = count + 1\n",
    "    return dist/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b661336d-ed69-4dc9-84d2-b9c8ec3cdd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9844786069297662"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_jaccard_diversity(topics, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815dee0-f1eb-4876-8917-ddc52b6dd25b",
   "metadata": {},
   "source": [
    "## Adding Topics to the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f582ef1-f6c3-4b99-a224-080189ec3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tweet_data_df = tweet_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c495d5ea-6ee8-42c9-9136-d33ba937a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tweet_data_df['topic_number'] = pruned_topic_df['Topic']\n",
    "labelled_tweet_data_df['topic_name'] = pruned_topic_df['Name']\n",
    "labelled_tweet_data_df['topic_words'] = pruned_topic_df['Top_n_words']\n",
    "labelled_tweet_data_df['topic_probabilities'] = pruned_topic_df['Probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5eaee02d-4220-4160-a526-d73f92cda7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tweet_data_df.to_csv('Results/pruned_topic_labelled_media_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7fd16-1d1c-4ff3-959c-06d05ae909ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ta)",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
