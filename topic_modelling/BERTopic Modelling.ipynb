{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eea8c2e-b329-48e7-9a2a-237c8f32aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "# Spacy's english model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# used to fix contractions such as I'll to I will\n",
    "import contractions\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137eba8a-6eb0-4117-8f4b-54907928b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "tweet_data_fp = '../model_evaluation/Results/pruned_labelled_network.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390d7629-e6dc-4e54-9496-db8b8a4794c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_df = pd.read_csv(tweet_data_fp)\n",
    "tweet_data_df = tweet_data_df[tweet_data_df['Lang_code'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f99876-7723-48d4-8123-c45c346bac07",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c189bca0-0e98-43af-9058-05ce7dc97977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering documents\n",
    "docs = tweet_data_df['Tweet'].tolist()\n",
    "pruned_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fed677dc-d95a-4425-abc3-0d4dff656277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27952"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64681897-7848-4981-b741-793375070aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best to err on the side of caution when pre-processing:\n",
    "# https://maartengr.github.io/BERTopic/faq.html#should-i-preprocess-the-data\n",
    "\n",
    "# Removal of links\n",
    "def remove_urls(doc):\n",
    "    return re.sub(r'http\\S+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_urls(doc) for doc in pruned_docs]\n",
    "\n",
    "# Converting hashtags into words\n",
    "# Or removing the hashtag and word entirely\n",
    "# Hashtags are converted to standard words as part of the tokenizer\n",
    "def remove_hashtags(doc):\n",
    "    return doc.replace('#', '')\n",
    "    #return re.sub(r'#\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_hashtags(doc) for doc in pruned_docs]\n",
    "\n",
    "# Converting mentions into words\n",
    "def remove_numbers(doc):\n",
    "    return re.sub(r'\\d+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_numbers(doc) for doc in pruned_docs]\n",
    "\n",
    "# Function to remove user mentions\n",
    "def remove_user_mentions(doc):\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_user_mentions(doc) for doc in pruned_docs]\n",
    "\n",
    "def fix_contractions(doc):\n",
    "    return contractions.fix(doc)\n",
    "\n",
    "pruned_docs = [fix_contractions(doc) for doc in pruned_docs]\n",
    "\n",
    "# Remove punctuation \n",
    "# Punctuation is removed as part of the tokenizer\n",
    "def remove_punctuation(doc):\n",
    "    return re.sub(r'[^\\w\\s]', '', doc)\n",
    "\n",
    "pruned_docs = [remove_punctuation(doc) for doc in pruned_docs]\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(doc):\n",
    "    return ' '.join([word for word in doc.split() if word.lower() not in stop_words])\n",
    "\n",
    "pruned_docs = [remove_stopwords(doc) for doc in pruned_docs]\n",
    "\n",
    "# Lemmatize the text\n",
    "def lemmatize_text(doc):\n",
    "    doc = nlp(doc)\n",
    "    return ' '.join([token.lemma_ for token in doc if token.lemma_ not in stop_words and not token.is_punct and not token.is_space])\n",
    "\n",
    "pruned_docs = [lemmatize_text(doc) for doc in pruned_docs]\n",
    "\n",
    "# Remove the 'amp' word\n",
    "def remove_amp(doc):\n",
    "    return re.sub(r'\\bamp\\b', '', doc).strip() # strip removes the surrounding white space\n",
    "\n",
    "pruned_docs = [remove_amp(doc) for doc in pruned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bf9efa6-0c1e-4d26-9947-f528158dd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda s: re.findall( '\\w+', s.lower() )\n",
    "\n",
    "pruned_docs_split = [ tokenizer(doc) for doc in  pruned_docs ]\n",
    "\n",
    "# Combine the inner lists into sentences\n",
    "pruned_docs_tokenized = [' '.join(words) for words in pruned_docs_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a47e15-2619-4960-be6c-7590dc179f2f",
   "metadata": {},
   "source": [
    "## Performing BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fa536cb-22b8-4134-8c99-7231e1eca3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# None of these give strong results\n",
    "#representation_model = KeyBERTInspired() \n",
    "#representation_model = PartOfSpeech(\"en_core_web_sm\") \n",
    "#representation_model = MaximalMarginalRelevance(diversity=0.8) \n",
    "\n",
    "# Topic modelling\n",
    "bertopic_model = BERTopic()\n",
    "topics, probs = bertopic_model.fit_transform(pruned_docs_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5adaba55-e623-4933-b254-37e54e88bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_topic_df = bertopic_model.get_document_info(pruned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd4bec5f-6fa8-431c-9a21-867ad3a92911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>speak Kids infection</td>\n",
       "      <td>174</td>\n",
       "      <td>174_kid_school_kids_pediatric</td>\n",
       "      <td>[kid, school, kids, pediatric, idph, sentiment...</td>\n",
       "      <td>[make sense since begin pandemic dr pedati idp...</td>\n",
       "      <td>kid - school - kids - pediatric - idph - senti...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>practice announce price tag notably deep globa...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>170</td>\n",
       "      <td>170_awesome_cool_indeed_woot</td>\n",
       "      <td>[awesome, cool, indeed, woot, haha, luck, grea...</td>\n",
       "      <td>[awesome, awesome, awesome]</td>\n",
       "      <td>awesome - cool - indeed - woot - haha - luck -...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good</td>\n",
       "      <td>215</td>\n",
       "      <td>215_good_ok_nice_thank</td>\n",
       "      <td>[good, ok, nice, thank, , , , , , ]</td>\n",
       "      <td>[good, good, good]</td>\n",
       "      <td>good - ok - nice - thank -  -  -  -  -  -</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whimper</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27947</th>\n",
       "      <td>receive question Michael thought utility rapid...</td>\n",
       "      <td>5</td>\n",
       "      <td>5_test_rapid_testing_athome</td>\n",
       "      <td>[test, rapid, testing, athome, free, tool, acc...</td>\n",
       "      <td>[want something help get athome rapid testing ...</td>\n",
       "      <td>test - rapid - testing - athome - free - tool ...</td>\n",
       "      <td>0.865229</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27948</th>\n",
       "      <td>create training hub boost pharmaceutical produ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27949</th>\n",
       "      <td>year ago today test US already fail diagnose p...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27950</th>\n",
       "      <td>OMG laugh hard Elmo replace Timothee Chalamet ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27951</th>\n",
       "      <td>sorry BGI unimpressed tell science publish paper</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_covid_vaccine_health_people</td>\n",
       "      <td>[covid, vaccine, health, people, pandemic, get...</td>\n",
       "      <td>[point vaccinate far far less risk serious dis...</td>\n",
       "      <td>covid - vaccine - health - people - pandemic -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27952 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Document  Topic  \\\n",
       "0                                   speak Kids infection    174   \n",
       "1      practice announce price tag notably deep globa...     -1   \n",
       "2                                                awesome    170   \n",
       "3                                                   good    215   \n",
       "4                                                whimper     -1   \n",
       "...                                                  ...    ...   \n",
       "27947  receive question Michael thought utility rapid...      5   \n",
       "27948  create training hub boost pharmaceutical produ...     -1   \n",
       "27949  year ago today test US already fail diagnose p...     -1   \n",
       "27950  OMG laugh hard Elmo replace Timothee Chalamet ...     -1   \n",
       "27951   sorry BGI unimpressed tell science publish paper     -1   \n",
       "\n",
       "                                 Name  \\\n",
       "0       174_kid_school_kids_pediatric   \n",
       "1      -1_covid_vaccine_health_people   \n",
       "2        170_awesome_cool_indeed_woot   \n",
       "3              215_good_ok_nice_thank   \n",
       "4      -1_covid_vaccine_health_people   \n",
       "...                               ...   \n",
       "27947     5_test_rapid_testing_athome   \n",
       "27948  -1_covid_vaccine_health_people   \n",
       "27949  -1_covid_vaccine_health_people   \n",
       "27950  -1_covid_vaccine_health_people   \n",
       "27951  -1_covid_vaccine_health_people   \n",
       "\n",
       "                                          Representation  \\\n",
       "0      [kid, school, kids, pediatric, idph, sentiment...   \n",
       "1      [covid, vaccine, health, people, pandemic, get...   \n",
       "2      [awesome, cool, indeed, woot, haha, luck, grea...   \n",
       "3                    [good, ok, nice, thank, , , , , , ]   \n",
       "4      [covid, vaccine, health, people, pandemic, get...   \n",
       "...                                                  ...   \n",
       "27947  [test, rapid, testing, athome, free, tool, acc...   \n",
       "27948  [covid, vaccine, health, people, pandemic, get...   \n",
       "27949  [covid, vaccine, health, people, pandemic, get...   \n",
       "27950  [covid, vaccine, health, people, pandemic, get...   \n",
       "27951  [covid, vaccine, health, people, pandemic, get...   \n",
       "\n",
       "                                     Representative_Docs  \\\n",
       "0      [make sense since begin pandemic dr pedati idp...   \n",
       "1      [point vaccinate far far less risk serious dis...   \n",
       "2                            [awesome, awesome, awesome]   \n",
       "3                                     [good, good, good]   \n",
       "4      [point vaccinate far far less risk serious dis...   \n",
       "...                                                  ...   \n",
       "27947  [want something help get athome rapid testing ...   \n",
       "27948  [point vaccinate far far less risk serious dis...   \n",
       "27949  [point vaccinate far far less risk serious dis...   \n",
       "27950  [point vaccinate far far less risk serious dis...   \n",
       "27951  [point vaccinate far far less risk serious dis...   \n",
       "\n",
       "                                             Top_n_words  Probability  \\\n",
       "0      kid - school - kids - pediatric - idph - senti...     1.000000   \n",
       "1      covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "2      awesome - cool - indeed - woot - haha - luck -...     1.000000   \n",
       "3             good - ok - nice - thank -  -  -  -  -  -      1.000000   \n",
       "4      covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "...                                                  ...          ...   \n",
       "27947  test - rapid - testing - athome - free - tool ...     0.865229   \n",
       "27948  covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "27949  covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "27950  covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "27951  covid - vaccine - health - people - pandemic -...     0.000000   \n",
       "\n",
       "       Representative_document  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                         True  \n",
       "3                         True  \n",
       "4                        False  \n",
       "...                        ...  \n",
       "27947                    False  \n",
       "27948                    False  \n",
       "27949                    False  \n",
       "27950                    False  \n",
       "27951                    False  \n",
       "\n",
       "[27952 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d940-4a06-4c60-b855-6bc61095dcab",
   "metadata": {},
   "source": [
    "## Calculating Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c0338e-7c65-4124-b85e-80666feea655",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = bertopic_model.get_topic_info()\n",
    "topics = topic_info['Representation'].tolist()\n",
    "\n",
    "word2id = Dictionary(pruned_docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed7b722a-330c-481e-aec5-14e26255e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(topics=topics, \n",
    "                    texts=pruned_docs_split,\n",
    "                    coherence='c_npmi',  # c_npmi was used in the bertopic serbian research paper\n",
    "                    dictionary=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c8c27c0-6196-4b2e-8e6d-6a14d4728299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "macro_topic_coherence = sum(coherence_per_topic) / len(coherence_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa2ecbd0-6b67-4ca7-b159-e8547221cc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04660587685604473"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This returns 0.02 when set to npmi, the serbian research return -0.042 as their best score\n",
    "# This returns 0.511 when set to c_v, this is a good coherence score\n",
    "# When removing non-english words and links this drops to 0.44\n",
    "# When adding lemmatization this goes up to 0.46\n",
    "# Removing numbers drop to 0.4567\n",
    "macro_topic_coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f74f6eae-11b2-4c2e-a820-d9e5b34ec451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    }
   ],
   "source": [
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be7ca0-ada5-47d3-b662-8e7233cdda4e",
   "metadata": {},
   "source": [
    "## Calculating Topic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0070bd1f-1b05-4f7a-8d3c-c62980f37da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found here: https://github.com/silviatti/topic-model-diversity/blob/master/diversity_metrics.py\n",
    "def proportion_unique_words(topics, topk=10):\n",
    "    \"\"\"\n",
    "    compute the proportion of unique words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity will be computed (There are 10 words in each topic)\n",
    "    \"\"\"\n",
    "    if topk > len(topics[0]):\n",
    "        raise Exception('Words in topics are less than '+str(topk))\n",
    "    else:\n",
    "        unique_words = set()\n",
    "        for topic in topics:\n",
    "            unique_words = unique_words.union(set(topic[:topk]))\n",
    "        puw = len(unique_words) / (topk * len(topics))\n",
    "        return puw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5de26953-bf2b-4641-b5bb-4d74cd4a8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def pairwise_jaccard_diversity(topics, topk=10):\n",
    "    '''\n",
    "    compute the average pairwise jaccard distance between the topics \n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    topics: a list of lists of words\n",
    "    topk: top k words on which the topic diversity\n",
    "          will be computed (There are 10 words in each topic)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pjd: average pairwise jaccard distance\n",
    "    '''\n",
    "    dist = 0\n",
    "    count = 0\n",
    "    for list1, list2 in combinations(topics, 2):\n",
    "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
    "        dist = dist + js\n",
    "        count = count + 1\n",
    "    return dist/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d3af1b0-d686-4e50-9b60-64e7e75d460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7817663817663818"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first inputs the topics, the second inputs the number of words in each topic\n",
    "proportion_unique_words(topics, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb47e8c9-121b-4ab1-9cc7-e1a580fa97d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988061700051492"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_jaccard_diversity(topics, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b9071-66d7-47d0-87d3-368858e9ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_info['Representation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665a506-14cc-464e-a31b-b2eb4c33022b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ta)",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
