{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0def05b4-12cf-4012-a63f-1db277dda194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "\n",
    "# Machine Learning\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17b951-06b5-46cf-8655-fa0f03b0237d",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0b99b46-a3ac-48a7-b51e-e994acc9355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "tweet_data_fp = 'twitter_data/custom_data/pruned_media_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19f65f20-982b-4eaf-b29a-7eb0d38cde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_df = pd.read_csv(tweet_data_fp)\n",
    "tweet_data_df = tweet_data_df[tweet_data_df['Lang_code'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dc0bc-217f-4a56-880b-d5c95926da3f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcfd78f2-8d19-47ba-849a-d40ff4e062b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering documents\n",
    "docs = tweet_data_df['Tweet'].tolist()\n",
    "pruned_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce17ccd2-fb36-4366-9436-ddd066f6fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of links\n",
    "def remove_urls(doc):\n",
    "    return re.sub(r'http\\S+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_urls(doc) for doc in pruned_docs]\n",
    "\n",
    "def convert_emojis(doc):\n",
    "    # delimiters are what is used around the emoji description, in this case spaces are used\n",
    "    return emoji.replace_emoji(doc, replace='')\n",
    "\n",
    "pruned_docs = [convert_emojis(doc) for doc in pruned_docs]\n",
    "\n",
    "def remove_hashtags(doc):\n",
    "    return doc.replace('#', '')\n",
    "    #return re.sub(r'#\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_hashtags(doc) for doc in pruned_docs]\n",
    "\n",
    "def remove_numbers(doc):\n",
    "    return re.sub(r'\\d+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_numbers(doc) for doc in pruned_docs]\n",
    "\n",
    "def remove_user_mentions(doc):\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "pruned_docs = [remove_user_mentions(doc) for doc in pruned_docs]\n",
    "\n",
    "def fix_contractions(doc):\n",
    "    return contractions.fix(doc)\n",
    "\n",
    "pruned_docs = [fix_contractions(doc) for doc in pruned_docs]\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    return re.sub(r'[^\\w\\s]', '', doc)\n",
    "\n",
    "pruned_docs = [remove_punctuation(doc) for doc in pruned_docs]\n",
    "\n",
    "def remove_amp(doc):\n",
    "    return re.sub(r'\\bamp\\b', '', doc).strip() # strip removes the surrounding white space\n",
    "\n",
    "pruned_docs = [remove_amp(doc) for doc in pruned_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09b336-8a00-42fa-90d7-e5795bfe1767",
   "metadata": {},
   "source": [
    "## Importing BERT Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4c6c8b5-d781-4885-860e-ceea288c383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to convert the texts to what is needed:\n",
    "# - turn the text into tensors\n",
    "# - truncate and pad the tweets to 280 characters\n",
    "def analyze_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=280)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64856d71-9690-432f-8084-ce5e166a8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads BERT tokenizer and model from a BERT model pre-trained on emotion dataset\n",
    "# Found here: https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "# This was based off of BERTweet which can be found here: https://github.com/VinAIResearch/BERTweet?tab=readme-ov-file\n",
    "# The model was then trained with SemEval 2017 corpus (around ~40k tweets) to refine sentiment\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentiment_model_results/checkpoint-1125')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sentiment_model_results/checkpoint-1125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12efc831-6480-4947-9f63-1dd6d3f1da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiments list\n",
    "sentiments = [\"negative\", \"neutral\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb90891f-b8c2-40c4-9347-2d5f2598faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through this campaign we wanted to challenge misinformation dispel fear and support the health workers and others at the forefront of the fight against the virus Ram Devineni the IndianAmerican creator of the Priya comic series said\n",
      "negative: 0.1185\n",
      "neutral: 0.8663\n",
      "positive: 0.0152\n",
      "\n",
      "Interesting news story Leaked documents reveal Chinas mishandling of the early stages of Covid\n",
      " pages of leaked documents from the Hubei Provincial Center for Disease Control and Prevention shared with and verified by CNN\n",
      "negative: 0.0034\n",
      "neutral: 0.0193\n",
      "positive: 0.9773\n",
      "\n",
      "These findings suggest that SARSCoV may have been introduced into the United States prior to January   Serologic testing of YOUS blood donations to identify SARSCoVreactive antibodies Dec Jan   Clinical Infectious Diseases  Oxford\n",
      "negative: 0.2633\n",
      "neutral: 0.7247\n",
      "positive: 0.0120\n",
      "\n",
      "Interesting perspective from a respected vaccine expert Dr Paul Offit\n",
      "negative: 0.0027\n",
      "neutral: 0.0121\n",
      "positive: 0.9852\n",
      "\n",
      "Gov Hogan Worst part of this entire crisis is still ahead of us in Maryland  Maryland hospitals now have a week to get a plan ready for the looming surge\n",
      "negative: 0.9845\n",
      "neutral: 0.0125\n",
      "positive: 0.0031\n",
      "\n",
      "FBI warns of new coronavirus email autoforwarding scam\n",
      "negative: 0.2457\n",
      "neutral: 0.7446\n",
      "positive: 0.0096\n",
      "\n",
      "Austin mayor stressed residents need to stay home He was vacationing in Cabo at the time SmartNews\n",
      "negative: 0.1354\n",
      "neutral: 0.8497\n",
      "positive: 0.0149\n",
      "\n",
      "The abrupt halt in world trade and tourism and the impact of lockdowns on international migration and remittances dealt a ruinous blow\n",
      "negative: 0.1537\n",
      "neutral: 0.8328\n",
      "positive: 0.0135\n",
      "\n",
      "Coronavirus Hackers targeted Covid vaccine supply cold chain phishing emails were sent out across six countries which targeted organisations linked to Cold Chain Equipment Optimisation Platform of Gavi the international vaccine alliance BBC News\n",
      "negative: 0.9842\n",
      "neutral: 0.0127\n",
      "positive: 0.0031\n",
      "\n",
      "Many clinicians worry about the toll that widespread public doubts and misinformation about the coronavirus are taking on their institutions overall ability to provide medical care\n",
      "negative: 0.0029\n",
      "neutral: 0.0232\n",
      "positive: 0.9740\n",
      "\n",
      "DC attacker struck woman in the face with a rock repeatedly shouted shut up and stop fighting cops looking for suspect in  block Wisconsin Ave attack photos\n",
      "negative: 0.4730\n",
      "neutral: 0.5135\n",
      "positive: 0.0135\n",
      "\n",
      "Health Officials Work to Solve Chinas Mystery Virus Outbreak  WSJ January  one of the first media reports on COVID\n",
      "negative: 0.1063\n",
      "neutral: 0.8795\n",
      "positive: 0.0142\n",
      "\n",
      "Transition cochair and former Obama administration official Jeff Zients is set to serve as the White Houses Covid coordinator and Vivek Murthy the former YOUS surgeon general under Obama will return to that role\n",
      "negative: 0.1403\n",
      "neutral: 0.8455\n",
      "positive: 0.0142\n",
      "\n",
      "Swiss report  new coronavirus cases in a day  Swiss civil protection agents are deployed to help during the coronavirus disease COVID outbreak in Meyrin near Geneva Switzerland\n",
      "negative: 0.2164\n",
      "neutral: 0.7726\n",
      "positive: 0.0110\n",
      "\n",
      "Risk perceptions and communication  Communication strategies in the context of COVID  PAHOWHO webinar today\n",
      "negative: 0.0994\n",
      "neutral: 0.8867\n",
      "positive: 0.0139\n",
      "\n",
      "We lost everything Central Americans flee north after hurricanes Backtoback hurricanes Eta and Iota internally displaced more than half a million people in Guatemala Honduras and Nicaragua according to the International Organization for Migration\n",
      "negative: 0.9788\n",
      "neutral: 0.0175\n",
      "positive: 0.0036\n",
      "\n",
      "So happy to see such comprehensive guidance Glad to see  out front But we are hungry for details There is mass confusion We need even more knowledge more detail more public face time more best practices from our now unshackled public health agency\n",
      "negative: 0.0027\n",
      "neutral: 0.0126\n",
      "positive: 0.9847\n",
      "\n",
      "It is great to see CDC as  puts it awaken from their politicsinduced coma I firmly believe we will not see and end to this pandemic until we end our  state solution and implement a comprehensive NATIONAL approach Having CDC in driver seat is critical\n",
      "negative: 0.0026\n",
      "neutral: 0.0119\n",
      "positive: 0.9855\n",
      "\n",
      "Distance is the most effective intervention the virus does not have legs so if you are physically distant from people you avoid direct contact and droplets\n",
      "negative: 0.0035\n",
      "neutral: 0.0219\n",
      "positive: 0.9746\n",
      "\n",
      "studies show the impact of Covid on women is greater than it is on men in terms of the burden of care the risk of exposure from work and economic hardship  more women than men are leaving the workforce due to the pandemic and some will never return\n",
      "negative: 0.9802\n",
      "neutral: 0.0166\n",
      "positive: 0.0032\n",
      "\n",
      "How  Epidemiologists Are Living Now and What They Think Is Next half of those surveyed said they would not change their personal behavior until at least  percent of the population was vaccinated\n",
      "negative: 0.9834\n",
      "neutral: 0.0136\n",
      "positive: 0.0030\n",
      "\n",
      "Intensive care units ICU at some German hospitals are reaching full capacity with seriously ill COVID patients the countrys hospital federation said A high percentage of nurses are off sick or in quarantine\n",
      "negative: 0.9842\n",
      "neutral: 0.0127\n",
      "positive: 0.0031\n",
      "\n",
      "Generation COVID Virus has changed young peoples lives\n",
      "negative: 0.0485\n",
      "neutral: 0.8304\n",
      "positive: 0.1211\n",
      "\n",
      "Bad news everywhere The coronavirus pandemic is an extraordinary global crisis Why do we feel there is bad news everywhere and what can we do about it\n",
      "negative: 0.9842\n",
      "neutral: 0.0127\n",
      "positive: 0.0032\n",
      "\n",
      "Microwave radiation is the most plausible because of migraines dizziness memory loss and other ailments that dozens of YOUS diplomats have complained of while serving in Cuba and China a new report says\n",
      "negative: 0.7449\n",
      "neutral: 0.2155\n",
      "positive: 0.0396\n",
      "\n",
      "Impressive numbers on PAHO support to countries of Latin America and the Caribbean\n",
      "negative: 0.0026\n",
      "neutral: 0.0137\n",
      "positive: 0.9837\n",
      "\n",
      "I also wish DA Henderson was here  to help us deal with COVID but I am not sure even he could have controlled the pandemic  Superman Where Are You Now  Tracking Zebra\n",
      "negative: 0.0032\n",
      "neutral: 0.0220\n",
      "positive: 0.9748\n",
      "\n",
      "Infections are emerging from modestsized gatherings of family and friends in a home including gatherings of around  people something the American public should be very careful about in the coming weeks Fauci said\n",
      "negative: 0.9831\n",
      "neutral: 0.0142\n",
      "positive: 0.0027\n",
      "\n",
      "Dr Rochelle Walensky new CDC director teaches at Harvard Medical School and is an infectious disease physician at both Massachusetts General Hospital and the Brigham and Womens Hospital in Boston\n",
      "negative: 0.2135\n",
      "neutral: 0.7754\n",
      "positive: 0.0112\n",
      "\n",
      "Not a naive take In our survey experiments vaccine authorization in Dec vs  wk preelection increased confidence  Faucis endorsement hadimpact Also as you say early ve experience could help\n",
      "\n",
      "But higher acceptance doesnt necessarily mean high enough acceptance\n",
      "negative: 0.9815\n",
      "neutral: 0.0159\n",
      "positive: 0.0026\n",
      "\n",
      "New head of new WHO Foundation\n",
      "negative: 0.1709\n",
      "neutral: 0.8179\n",
      "positive: 0.0113\n",
      "\n",
      "William Shakespeare gets Covid vaccine Alls well that ends well\n",
      "negative: 0.0933\n",
      "neutral: 0.8910\n",
      "positive: 0.0156\n",
      "\n",
      "Interesting\n",
      "negative: 0.0028\n",
      "neutral: 0.0112\n",
      "positive: 0.9860\n",
      "\n",
      "Red wave map shows how US COVID hospitalizations have spread\n",
      "negative: 0.9741\n",
      "neutral: 0.0237\n",
      "positive: 0.0022\n",
      "\n",
      "Well deserved award for  of\n",
      "negative: 0.0045\n",
      "neutral: 0.0604\n",
      "positive: 0.9352\n",
      "\n",
      "Still a long way to go\n",
      "negative: 0.9831\n",
      "neutral: 0.0142\n",
      "positive: 0.0027\n",
      "\n",
      "As YOUS companies push to get workers vaccinated states disagree on who is essential plans show broad discrepancies over who would be considered essential with some states clearly outlining specific worker groups and others not providing any clarity\n",
      "negative: 0.4832\n",
      "neutral: 0.5023\n",
      "positive: 0.0145\n",
      "\n",
      "Community health workers are at the heart of the COVID response in Dominicas Roseau Health District Bringing health care closer to the community through primary health care and training the health workforce in peoplecentered\n",
      "negative: 0.1282\n",
      "neutral: 0.8571\n",
      "positive: 0.0147\n",
      "\n",
      "Logistics security for vaccine shipments  The YOUK is an ideal test for the roll out a developed country not far from the production site with a good health service and a dense population\n",
      "negative: 0.0028\n",
      "neutral: 0.0130\n",
      "positive: 0.9843\n",
      "\n",
      "Oklahoma woman with coronavirus received  false negatives before positive test Listen to your gut\n",
      "negative: 0.9843\n",
      "neutral: 0.0127\n",
      "positive: 0.0030\n",
      "\n",
      "Montgomery Co may again close indoor dining as Maryland sees astronomical case rates SmartNews\n",
      "negative: 0.1981\n",
      "neutral: 0.7875\n",
      "positive: 0.0144\n",
      "\n",
      "Pro tip do not go on a cruise  with  strangers during a pandemic and bring your year old grandpa along COVID\n",
      "negative: 0.6965\n",
      "neutral: 0.2971\n",
      "positive: 0.0064\n",
      "\n",
      "Imagine that\n",
      "negative: 0.0678\n",
      "neutral: 0.9140\n",
      "positive: 0.0182\n",
      "\n",
      "air\n",
      "o\n",
      "seoul\n",
      "negative: 0.0845\n",
      "neutral: 0.9003\n",
      "positive: 0.0152\n",
      "\n",
      "We knew this at least since the summer\n",
      "negative: 0.9840\n",
      "neutral: 0.0131\n",
      "positive: 0.0029\n",
      "\n",
      "Essentially they find that Facebook is a powerful social media monopoly that collects a massive amount of data on American users that the company uses to sell advertisements\n",
      "negative: 0.0185\n",
      "neutral: 0.3279\n",
      "positive: 0.6536\n",
      "\n",
      "Smallpox and the photos antivaxxers do not want you to see  NZ Herald SmartNews\n",
      "negative: 0.1012\n",
      "neutral: 0.8846\n",
      "positive: 0.0142\n",
      "\n",
      "Complete coverage of the FDA committee looking at the vaccine today with slides votes presentations etc  Great thread by\n",
      "negative: 0.0027\n",
      "neutral: 0.0109\n",
      "positive: 0.9863\n",
      "\n",
      "Interesting insights into COVID messaging communications trust and the  patterns of response by groups of countries Keep it away hold it at bay Surge and Ebb and  Surge and surge again Richard Edelman says There\n",
      "negative: 0.0025\n",
      "neutral: 0.0135\n",
      "positive: 0.9839\n",
      "\n",
      "Swiss impose pm shutdown on most shops and restaurants to tackle COVID Government says Hospitals are close to their limits and health personnel are under pressure The situation is disturbing\n",
      "negative: 0.2203\n",
      "neutral: 0.5488\n",
      "positive: 0.2309\n",
      "\n",
      "Outstanding conversation with  experts on COVIDVaccine  distribution safety etc in Spanish\n",
      "negative: 0.0026\n",
      "neutral: 0.0123\n",
      "positive: 0.9851\n",
      "\n",
      "What matters for vaccine decision making is to properly assess risk vs  benefit Fro smallpox it is clear the vaccine was the solution\n",
      "negative: 0.0753\n",
      "neutral: 0.8448\n",
      "positive: 0.0798\n",
      "\n",
      "Vaccination is an additional tool in the fight against the COVID pandemic not the only one and it does not negate the need to continue using all the other tools to box in the virus masks test and trace isolate etc\n",
      "negative: 0.9837\n",
      "neutral: 0.0134\n",
      "positive: 0.0029\n",
      "\n",
      "The biggest challenges to the Covid vaccine rollout Cox said are trust in institutions perceived safety of the vaccine real safety of the vaccine and access to the vaccine Elemental has attempted to address all four of those  Tara Haelle\n",
      "negative: 0.9547\n",
      "neutral: 0.0416\n",
      "positive: 0.0037\n",
      "\n",
      "Great update on Covid vaccines from PAHO experts their safety distribution logistics and more In Spanish\n",
      "negative: 0.0027\n",
      "neutral: 0.0113\n",
      "positive: 0.9860\n",
      "\n",
      "Touch screens at airports are dirtier than anyplace in your house\n",
      "negative: 0.9810\n",
      "neutral: 0.0164\n",
      "positive: 0.0026\n",
      "\n",
      "Limited availability of approved vaccines dims hopes of imminent end to pandemic economic distress\n",
      "negative: 0.9792\n",
      "neutral: 0.0182\n",
      "positive: 0.0026\n",
      "\n",
      "Travel restrictions at YOUS land borders extended through January  White House considering rescinding European and Brazilian entry bans\n",
      "negative: 0.1424\n",
      "neutral: 0.8440\n",
      "positive: 0.0135\n",
      "\n",
      "South Koreas successful approach of regimented masking aggressive testing and hightech contact tracing is a blueprint for the YOUS and other democracies\n",
      "negative: 0.3818\n",
      "neutral: 0.4444\n",
      "positive: 0.1739\n",
      "\n",
      "Official word from the FDA\n",
      "negative: 0.0978\n",
      "neutral: 0.8887\n",
      "positive: 0.0135\n",
      "\n",
      "The Next Decade Could Be Even Worse  Havoc at the level of the late s and early s is the bestcase scenario allout civil war is the worst says this ecological historian  The Atlantic\n",
      "negative: 0.9837\n",
      "neutral: 0.0131\n",
      "positive: 0.0032\n",
      "\n",
      "US authorization of first COVID vaccine marks new phase in safety monitoring\n",
      "negative: 0.1155\n",
      "neutral: 0.8710\n",
      "positive: 0.0134\n",
      "\n",
      "Interesting from NIH scientist\n",
      "negative: 0.0026\n",
      "neutral: 0.0118\n",
      "positive: 0.9856\n",
      "\n",
      "Brazils staterun Fiocruz Institute scientists collect and study viruses present in wild animals  including bats which many scientists believe were linked to the outbreak of COVID   \n",
      "Scientists focus on bats for clues to prevent next pandemic\n",
      "negative: 0.0034\n",
      "neutral: 0.0317\n",
      "positive: 0.9649\n",
      "\n",
      "From Public Health Agency of Canada\n",
      "negative: 0.0900\n",
      "neutral: 0.8956\n",
      "positive: 0.0144\n",
      "\n",
      "Canada announces additional support for equitable access to COVID tests treatments and vaccines\n",
      "negative: 0.0947\n",
      "neutral: 0.8800\n",
      "positive: 0.0253\n",
      "\n",
      "The first YOUS Covid vaccinations outside of clinical trials began Monday kicking off the most urgent mass immunization campaign since polio shots were rolled out in the s\n",
      "negative: 0.0158\n",
      "neutral: 0.3290\n",
      "positive: 0.6552\n",
      "\n",
      "Understanding Consumers Responses to Threats Interesting research on Covid_ and persuasion from another perspective  Journal of Consumer Research  Oxford Academic\n",
      "negative: 0.0026\n",
      "neutral: 0.0128\n",
      "positive: 0.9846\n",
      "\n",
      "Oracles Ellison I have moved to the state of Hawaii and I will be using the power of Zoom to work from the island of Lanai  Ellison owns almost the entire island of Lanai which he has tried to develop as an environmental and agricultural utopia\n",
      "negative: 0.1023\n",
      "neutral: 0.8804\n",
      "positive: 0.0174\n",
      "\n",
      "Scientists also do not yet have the data to confirm that the vaccines actually prevent people from spreading the coronavirus asymptomatically in addition to preventing COVID symptoms\n",
      "negative: 0.1126\n",
      "neutral: 0.8730\n",
      "positive: 0.0144\n",
      "\n",
      "Good QampA on new vaccines\n",
      "negative: 0.0028\n",
      "neutral: 0.0109\n",
      "positive: 0.9863\n",
      "\n",
      "World Health Organization Epidemiologist Only with the Vaccination of Younger People Will the Number of Cases Decrease  great interview\n",
      "negative: 0.0028\n",
      "neutral: 0.0117\n",
      "positive: 0.9856\n",
      "\n",
      "This is encouraging but I do worry about the possibility that people may wait too long to get vaccinated\n",
      "negative: 0.9826\n",
      "neutral: 0.0147\n",
      "positive: 0.0026\n",
      "\n",
      "Canada funding for COVID in developing countries\n",
      "negative: 0.0785\n",
      "neutral: 0.9058\n",
      "positive: 0.0157\n",
      "\n",
      "Marylands COVID vaccination plans outlined in  press briefing\n",
      "negative: 0.0955\n",
      "neutral: 0.8906\n",
      "positive: 0.0139\n",
      "\n",
      "Maryland Montgomery County bans indoor dining limits capacity in stores county is relying on data from Johns Hopkins that says stricter measures on indoor dining will decrease hospitalizations by  to  percent SmartNews\n",
      "negative: 0.1438\n",
      "neutral: 0.8421\n",
      "positive: 0.0141\n",
      "\n",
      "Interesting\n",
      "negative: 0.0028\n",
      "neutral: 0.0112\n",
      "positive: 0.9860\n",
      "\n",
      "This tool will calculate your risk of mortality from COVID in currently uninfected individuals based on input on a set of riskfactors and communitylevel pandemic dynamics in the state of residence\n",
      "negative: 0.2618\n",
      "neutral: 0.7260\n",
      "positive: 0.0122\n",
      "\n",
      "Try out the calculator\n",
      "negative: 0.0741\n",
      "neutral: 0.9109\n",
      "positive: 0.0150\n",
      "\n",
      "For children wondering about COVID and Santa Claus\n",
      "negative: 0.0941\n",
      "neutral: 0.8900\n",
      "positive: 0.0159\n",
      "\n",
      "Yes but probably not until\n",
      "negative: 0.1052\n",
      "neutral: 0.8805\n",
      "positive: 0.0143\n",
      "\n",
      "At some point in  vaccine will be sufficiently available that anybody who wants to be vaccinated will have been able to get vaccinated he predicts\n",
      "negative: 0.0028\n",
      "neutral: 0.0166\n",
      "positive: 0.9806\n",
      "\n",
      "Sad poignant thread about the death of  s mother one of  deaths worldwide caused by this terrible virus to date\n",
      "negative: 0.9842\n",
      "neutral: 0.0126\n",
      "positive: 0.0032\n",
      "\n",
      "The virus that first emerged a year ago in Wuhan China swept across the world in  leaving havoc in its wake  The pandemic has been a global event leaving joblessness and lockdowns infirmity and death And an abiding relentless fear\n",
      "negative: 0.1391\n",
      "neutral: 0.8465\n",
      "positive: 0.0144\n",
      "\n",
      "Twitter to expand COVID vaccines policy to combat harmful misleading tweets\n",
      "negative: 0.9843\n",
      "neutral: 0.0128\n",
      "positive: 0.0029\n",
      "\n",
      "American Teen and Boyfriend Sentenced to  Months in Prison for Breaking Quarantine in Cayman Islands SmartNews\n",
      "negative: 0.1751\n",
      "neutral: 0.8122\n",
      "positive: 0.0127\n",
      "\n",
      "Some experts predict it will be  before there is enough vaccine Others like Dr Hatchett think that as more people get sick and acquire natural immunity the need for the vaccine will diminish and the supply will be adequate by late\n",
      "negative: 0.9820\n",
      "neutral: 0.0146\n",
      "positive: 0.0034\n",
      "\n",
      "Graciela Cadet MBA Her commitment to training staff on safe and effective clinical care standards shows her dedication to improving health care delivery for patients in Haiti\n",
      "negative: 0.0029\n",
      "neutral: 0.0189\n",
      "positive: 0.9782\n",
      "\n",
      "Do not be a holiday superspreader of covid\n",
      "negative: 0.0963\n",
      "neutral: 0.8857\n",
      "positive: 0.0180\n",
      "\n",
      "The who is chief scientist on a year of loss and learning I am proud of many things my team has achieved in the past  months even as the shortcomings of both the WHO and the global community have been laid bare\n",
      "negative: 0.1257\n",
      "neutral: 0.4798\n",
      "positive: 0.3944\n",
      "\n",
      "Philanthropy The result over the last four months has been  in gifts to  organizations across all  states Puerto Rico and Washington DC\n",
      "negative: 0.1149\n",
      "neutral: 0.8716\n",
      "positive: 0.0135\n",
      "\n",
      "Switzerland Restaurants and bars will be required to close No exceptions will be made for the festive period Sports facilities to close Cultural and leisure venues to close\n",
      "Further restrictions on capacity of shops  Restrictions on opening hours continue to apply\n",
      "negative: 0.3136\n",
      "neutral: 0.6731\n",
      "positive: 0.0134\n",
      "\n",
      "Even before COVID flipped everything upsidedown employees attention spans were quickly waning \n",
      " words Length of message where  of people will stop reading\n",
      " seconds Average attention span for digital tasks\n",
      " hour\n",
      "negative: 0.9828\n",
      "neutral: 0.0146\n",
      "positive: 0.0026\n",
      "\n",
      "New COVAX deals An advance purchase agreement with AstraZeneca for  million doses of the AstraZenecaOxford candidate and a memorandum of understanding with Johnson  Johnson for  million doses of the Janssen candidate cur\n",
      "negative: 0.1381\n",
      "neutral: 0.8474\n",
      "positive: 0.0145\n",
      "\n",
      "Normal will not return for a long time But in the coming months as vaccines are rolled out and a fuller picture of their promise emerges we may finally be able to answer the question When is this going to end\n",
      "negative: 0.9061\n",
      "neutral: 0.0852\n",
      "positive: 0.0087\n",
      "\n",
      "Pfizer irked after Belgian politician publishes COVID vaccine prices  Belgian government paid  euros  per dose to buy about five million shots of the PfizerBioNTech vaccine  Reuters\n",
      "negative: 0.1976\n",
      "neutral: 0.7865\n",
      "positive: 0.0159\n",
      "\n",
      "You could be infected and you could be contagious even though you had the vaccine said Dr William Schaffner\n",
      "negative: 0.0991\n",
      "neutral: 0.8852\n",
      "positive: 0.0157\n",
      "\n",
      "COVID vaccine makers tap contractors to produce billions of doses interesting look at production and supply chain arrangements\n",
      "negative: 0.0026\n",
      "neutral: 0.0126\n",
      "positive: 0.9848\n",
      "\n",
      "Turns out we will need all the support we can get to reach herd immunity or as my boys put it find a way to save the world\n",
      "negative: 0.0974\n",
      "neutral: 0.8876\n",
      "positive: 0.0151\n",
      "\n",
      "MD Dining Bans Prompt Restaurant Group To Sue Jurisdictions SmartNews\n",
      "negative: 0.0972\n",
      "neutral: 0.8877\n",
      "positive: 0.0151\n",
      "\n",
      "These are useful pokes for scientists and governments to get systems in place  now before we might need them especially as we start vaccinating people Dr Hodcroft said\n",
      "\n",
      "Great overview of mutation vaccination  what scientists concerns are\n",
      "negative: 0.0026\n",
      "neutral: 0.0124\n",
      "positive: 0.9850\n",
      "\n",
      "Here  puts the news about SARSCoV mutations from the UK and South Africa in context Worth tracking and studying these genomic changes but no need for alarm just yet\n",
      "negative: 0.0751\n",
      "neutral: 0.8966\n",
      "positive: 0.0284\n",
      "\n",
      "MD  Case Trends by Zip code Total cases in   In last  days  mSightly Data Hub\n",
      "negative: 0.1103\n",
      "neutral: 0.8755\n",
      "positive: 0.0142\n",
      "\n",
      "The COVID situation in Maryland\n",
      "negative: 0.0789\n",
      "neutral: 0.9063\n",
      "positive: 0.0148\n",
      "\n",
      "Threat Assessment Brief Rapid increase of a SARSCoV variant with multiple spike protein mutations observed in the United Kingdom There is no indication at this point of increased infection severity associated with the new variant\n",
      "negative: 0.1898\n",
      "neutral: 0.7966\n",
      "positive: 0.0136\n",
      "\n",
      "As usual  covers all things infectious diseases thoroughly for  and does masterful twitter threads Today it is on COVID vaccines\n",
      "negative: 0.9507\n",
      "neutral: 0.0431\n",
      "positive: 0.0062\n",
      "\n",
      "UK variant\n",
      "negative: 0.0774\n",
      "neutral: 0.9053\n",
      "positive: 0.0173\n",
      "\n",
      "Good interview by  epidemiologist on UK virus variant\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m pruned_docs:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doc)\n\u001b[0;32m----> 4\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentiments):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobabilities[\u001b[38;5;241m0\u001b[39m][i]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_sentiment\u001b[39m(text):\n\u001b[1;32m      5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m280\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      8\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1191\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:828\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    821\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    822\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    827\u001b[0m )\n\u001b[0;32m--> 828\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    841\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:517\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    506\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    507\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    508\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m         output_attentions,\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:406\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    396\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:333\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 333\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:259\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    256\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ta/lib/python3.8/site-packages/torch/nn/functional.py:1885\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Predicting sentiment for the individual tweets\n",
    "for doc in pruned_docs:\n",
    "    print(doc)\n",
    "    probabilities = analyze_sentiment(doc)\n",
    "    for i, label in enumerate(sentiments):\n",
    "        print(f\"{label}: {probabilities[0][i].item():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d240d2-ea13-4d66-a1a8-8939a72de1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Through this campaign we wanted to challenge misinformation dispel fear and support the health workers and others at the forefront of the fight against the virus Ram Devineni the IndianAmerican creator of the Priya comic series said\n",
    "negative: 0.0044\n",
    "neutral: 0.3677\n",
    "positive: 0.6280\n",
    "\n",
    "Interesting news story Leaked documents reveal Chinas mishandling of the early stages of Covid\n",
    " pages of leaked documents from the Hubei Provincial Center for Disease Control and Prevention shared with and verified by CNN\n",
    "negative: 0.1531\n",
    "neutral: 0.8284\n",
    "positive: 0.0185\n",
    "\n",
    "These findings suggest that SARSCoV may have been introduced into the United States prior to January   Serologic testing of YOUS blood donations to identify SARSCoVreactive antibodies Dec Jan   Clinical Infectious Diseases  Oxford\n",
    "negative: 0.0144\n",
    "neutral: 0.9644\n",
    "positive: 0.0212\n",
    "\n",
    "Interesting perspective from a respected vaccine expert Dr Paul Offit\n",
    "negative: 0.0015\n",
    "neutral: 0.1047\n",
    "positive: 0.8938\n",
    "\n",
    "Gov Hogan Worst part of this entire crisis is still ahead of us in Maryland  Maryland hospitals now have a week to get a plan ready for the looming surge\n",
    "negative: 0.9605\n",
    "neutral: 0.0370\n",
    "positive: 0.0025\n",
    "\n",
    "FBI warns of new coronavirus email autoforwarding scam\n",
    "negative: 0.8956\n",
    "neutral: 0.1008\n",
    "positive: 0.0037\n",
    "\n",
    "Austin mayor stressed residents need to stay home He was vacationing in Cabo at the time SmartNews\n",
    "negative: 0.4334\n",
    "neutral: 0.5604\n",
    "positive: 0.0062\n",
    "\n",
    "The abrupt halt in world trade and tourism and the impact of lockdowns on international migration and remittances dealt a ruinous blow\n",
    "negative: 0.9611\n",
    "neutral: 0.0364\n",
    "positive: 0.0025\n",
    "\n",
    "Coronavirus Hackers targeted Covid vaccine supply cold chain phishing emails were sent out across six countries which targeted organisations linked to Cold Chain Equipment Optimisation Platform of Gavi the international vaccine alliance BBC News\n",
    "negative: 0.8842\n",
    "neutral: 0.1123\n",
    "positive: 0.0035\n",
    "\n",
    "Many clinicians worry about the toll that widespread public doubts and misinformation about the coronavirus are taking on their institutions overall ability to provide medical care\n",
    "negative: 0.7934\n",
    "neutral: 0.2007\n",
    "positive: 0.0059\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ta)",
   "language": "python",
   "name": "ta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
